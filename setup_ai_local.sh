#!/bin/bash

echo "ğŸš€ Configurando JARVIS 3.0 com IA Local Ollama"
echo "=============================================="

# Parar containers existentes
echo "ğŸ“¦ Parando containers existentes..."
docker stop Ollama_IA_LOCAL Jarvis_WebUI Jarvis_Training 2>/dev/null || true

# Iniciar nova stack completa
echo "ğŸ—ï¸ Iniciando stack completa..."
docker-compose up -d

# Aguardar Ollama ficar pronto
echo "â³ Aguardando Ollama inicializar..."
sleep 10

# Verificar se modelos estÃ£o disponÃ­veis
echo "ğŸ§  Verificando modelos..."
docker exec Ollama_IA_LOCAL ollama list

# Criar modelo personalizado Jarvis
echo "ğŸ¯ Criando modelo personalizado do Jarvis..."
docker exec Ollama_IA_LOCAL ollama create jarvis-personal -f /training_data/Modelfile

# Baixar modelos adicionais se necessÃ¡rio
echo "ğŸ“¥ Verificando modelos disponÃ­veis..."
models=$(docker exec Ollama_IA_LOCAL ollama list | wc -l)
if [ $models -lt 3 ]; then
    echo "ğŸ“¥ Baixando modelo adicional..."
    docker exec Ollama_IA_LOCAL ollama pull llama3.2:3b
fi

echo ""
echo "âœ… CONFIGURAÃ‡ÃƒO CONCLUÃDA!"
echo "========================="
echo ""
echo "ğŸŒ Acessos disponÃ­veis:"
echo "   â€¢ WebUI (ChatGPT-like): http://localhost:3000"
echo "   â€¢ Jupyter (Treinamento): http://localhost:8888 (token: jarvis2025)"
echo "   â€¢ Ollama API: http://localhost:11434"
echo ""
echo "ğŸ¤– Modelos disponÃ­veis:"
docker exec Ollama_IA_LOCAL ollama list
echo ""
echo "ğŸ”§ Comandos Ãºteis:"
echo "   â€¢ Ver logs: docker-compose logs -f"
echo "   â€¢ Parar tudo: docker-compose down"
echo "   â€¢ Chat direto: docker exec -it Ollama_IA_LOCAL ollama run jarvis-personal"
echo ""
echo "ğŸ‰ Seu Jarvis estÃ¡ pronto para usar!"
