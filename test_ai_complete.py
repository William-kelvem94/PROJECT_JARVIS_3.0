#!/usr/bin/env python3
"""
ğŸ§  Teste Completo da IntegraÃ§Ã£o IA Local do Jarvis 3.0
Valida Ollama, WebUI, e integraÃ§Ã£o Python
"""

import asyncio
import sys
import os
sys.path.append('NEW_JARVIS')

from core.ai_engine import AIEngine
from core.config import Config
from core.ollama_integration import OllamaLocalAI

def test_ollama_direct():
    """Teste direto da integraÃ§Ã£o Ollama"""
    print("ğŸ” Testando Ollama Direct...")
    
    ollama = OllamaLocalAI()
    
    # Verificar conexÃ£o
    if not ollama.check_connection():
        print("âŒ Ollama nÃ£o estÃ¡ rodando!")
        return False
    
    print("âœ… Ollama conectado!")
    
    # Listar modelos
    models = ollama.list_models()
    print(f"ğŸ“‹ Modelos disponÃ­veis: {len(models)}")
    for model in models:
        print(f"   â€¢ {model['name']} ({model.get('size', 'N/A')})")
    
    # Testar com modelo personalizado
    if ollama.set_model("jarvis-personal"):
        print("ğŸ¯ Usando modelo personalizado Jarvis")
    else:
        print("âš ï¸ Modelo personalizado nÃ£o encontrado, usando padrÃ£o")
        ollama.set_model("llama3.2:1b")
    
    # Teste de conversa
    print("\nğŸ’¬ Teste de Conversa:")
    test_messages = [
        "OlÃ¡! VocÃª Ã© o Jarvis?",
        "Qual Ã© o status do sistema?",
        "Me fale sobre suas capacidades"
    ]
    
    for msg in test_messages:
        print(f"\nğŸ‘¤ UsuÃ¡rio: {msg}")
        response = ollama.chat(msg)
        print(f"ğŸ¤– Jarvis: {response}")
    
    return True

async def test_ai_engine():
    """Teste do motor de IA completo"""
    print("\nğŸ¯ Testando AI Engine...")
    
    # ConfiguraÃ§Ã£o bÃ¡sica
    config = Config()
    ai_engine = AIEngine(config.ai)
    
    # Status da IA
    status = ai_engine.get_ai_status()
    print(f"ğŸ“Š Status IA:")
    print(f"   â€¢ Ollama: {'âœ…' if status['ollama_available'] else 'âŒ'}")
    print(f"   â€¢ OpenAI: {'âœ…' if status['openai_available'] else 'âŒ'}")
    print(f"   â€¢ Modo atual: {status['current_mode']}")
    print(f"   â€¢ Modelo atual: {status['current_model']}")
    
    # Teste de personalidades
    print(f"\nğŸ­ Testando Personalidades:")
    personalities = ["assistente", "amigavel", "tecnico"]
    
    for personality in personalities:
        ai_engine.set_personality(personality)
        response = await ai_engine.chat("Apresente-se brevemente")
        print(f"   â€¢ {personality}: {response[:100]}...")
    
    # Teste de comandos com contexto
    print(f"\nğŸ”§ Teste com Contexto do Sistema:")
    
    # Simular contexto do sistema
    system_context = {
        "cpu_usage": 45.2,
        "memory_usage": 68.1,
        "uptime": "2 horas, 15 minutos",
        "active_processes": 156
    }
    
    response = await ai_engine.chat(
        "Analise o desempenho atual do sistema", 
        context=system_context
    )
    print(f"ğŸ¤– AnÃ¡lise: {response}")
    
    return True

def test_web_access():
    """Testa acesso Ã s interfaces web"""
    print("\nğŸŒ Testando Interfaces Web...")
    
    import requests
    
    endpoints = [
        ("Ollama API", "http://localhost:11434/api/tags"),
        ("Open WebUI", "http://localhost:3000"),
        ("Jupyter", "http://localhost:8888")
    ]
    
    for name, url in endpoints:
        try:
            response = requests.get(url, timeout=5)
            status = "âœ…" if response.status_code == 200 else f"âš ï¸ ({response.status_code})"
            print(f"   â€¢ {name}: {status}")
        except Exception as e:
            print(f"   â€¢ {name}: âŒ {str(e)}")

def create_quick_demo():
    """Cria demonstraÃ§Ã£o rÃ¡pida do sistema"""
    print("\nğŸ¬ Criando Demo Interativo...")
    
    demo_content = """
# ğŸ§  Jarvis 3.0 - IA Local Demo

## ğŸš€ Endpoints DisponÃ­veis:
- **WebUI**: http://localhost:3000 (Interface tipo ChatGPT)
- **Jupyter**: http://localhost:8888 (token: jarvis2025)
- **Ollama API**: http://localhost:11434

## ğŸ¯ Modelos DisponÃ­veis:
- `jarvis-personal` - Modelo personalizado do Jarvis
- `llama3.2:1b` - Modelo base Llama

## ğŸ’¡ Exemplos de Uso:

### Via WebUI (Recomendado):
1. Abra http://localhost:3000
2. FaÃ§a login/cadastro
3. Selecione modelo "jarvis-personal"
4. Comece a conversar!

### Via Python:
```python
from core.ai_engine import AIEngine
from core.config import Config

config = Config()
ai = AIEngine(config.ai)

# Conversa simples
response = await ai.chat("OlÃ¡ Jarvis!")
print(response)

# Com contexto do sistema
context = {"cpu_usage": 45.2, "memory_usage": 68.1}
response = await ai.chat("Como estÃ¡ o sistema?", context=context)
print(response)
```

### Via API direta:
```bash
curl -X POST http://localhost:11434/api/generate -d '{
  "model": "jarvis-personal",
  "prompt": "OlÃ¡! VocÃª Ã© o Jarvis?",
  "stream": false
}'
```

## ğŸ­ Personalidades:
- **assistente**: Formal e profissional
- **amigavel**: DescontraÃ­do e casual  
- **tecnico**: Especialista em tecnologia

## ğŸ”§ Comandos Ãšteis:
```bash
# Status containers
docker ps

# Logs do Ollama
docker logs Ollama_IA_LOCAL

# Modelos disponÃ­veis
docker exec Ollama_IA_LOCAL ollama list

# Conversa via terminal
docker exec -it Ollama_IA_LOCAL ollama run jarvis-personal
```
"""
    
    with open("AI_LOCAL_DEMO.md", "w", encoding="utf-8") as f:
        f.write(demo_content)
    
    print("âœ… Demo criado: AI_LOCAL_DEMO.md")

async def main():
    """FunÃ§Ã£o principal de teste"""
    print("ğŸ§  JARVIS 3.0 - Teste Completo da IA Local\n")
    print("=" * 50)
    
    # Testes sequenciais
    if not test_ollama_direct():
        print("âŒ Falha no teste do Ollama")
        return
    
    if not await test_ai_engine():
        print("âŒ Falha no teste do AI Engine")
        return
    
    test_web_access()
    create_quick_demo()
    
    print("\n" + "=" * 50)
    print("ğŸ‰ TODOS OS TESTES CONCLUÃDOS!")
    print("\nğŸ“– Consulte AI_LOCAL_DEMO.md para exemplos de uso")
    print("ğŸŒ WebUI: http://localhost:3000")
    print("ğŸ“Š Jupyter: http://localhost:8888 (token: jarvis2025)")

if __name__ == "__main__":
    asyncio.run(main())
