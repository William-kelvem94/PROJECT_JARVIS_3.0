{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fd4a01f",
   "metadata": {},
   "source": [
    "# üß† Treinamento de IA Local para Jarvis 3.0\n",
    "\n",
    "Este notebook te ajuda a personalizar e treinar sua IA local usando Ollama.\n",
    "\n",
    "## üìã O que voc√™ pode fazer aqui:\n",
    "\n",
    "1. **Fine-tuning de Personalidade** - Treinar como o Jarvis responde\n",
    "2. **Injection de Conhecimento** - Adicionar informa√ß√µes espec√≠ficas\n",
    "3. **Teste de Modelos** - Comparar diferentes configura√ß√µes\n",
    "4. **An√°lise de Performance** - Medir qualidade das respostas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817bc809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Configura√ß√£o inicial\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Configurar conex√£o com Ollama\n",
    "OLLAMA_URL = \"http://ollama:11434\"  # URL dentro do container\n",
    "\n",
    "def test_connection():\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_URL}/api/tags\")\n",
    "        return response.status_code == 200\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "print(f\"üîó Conectando com Ollama: {'‚úÖ OK' if test_connection() else '‚ùå FALHOU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f008d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã Listar modelos dispon√≠veis\n",
    "def list_models():\n",
    "    response = requests.get(f\"{OLLAMA_URL}/api/tags\")\n",
    "    if response.status_code == 200:\n",
    "        models = response.json().get(\"models\", [])\n",
    "        return [(m[\"name\"], m[\"size\"]) for m in models]\n",
    "    return []\n",
    "\n",
    "models = list_models()\n",
    "print(\"ü§ñ Modelos dispon√≠veis:\")\n",
    "for name, size in models:\n",
    "    print(f\"   ‚Ä¢ {name} ({size//1000000000:.1f}GB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1df4be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üí¨ Fun√ß√£o para chat com qualquer modelo\n",
    "def chat_with_model(model_name, prompt, temperature=0.8):\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 40\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.post(f\"{OLLAMA_URL}/api/generate\", json=payload, timeout=30)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return {\n",
    "                \"response\": result.get(\"response\", \"\"),\n",
    "                \"time\": end_time - start_time,\n",
    "                \"success\": True\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"response\": f\"Erro: {e}\",\n",
    "            \"time\": 0,\n",
    "            \"success\": False\n",
    "        }\n",
    "\n",
    "# Testar modelo padr√£o\n",
    "test_result = chat_with_model(\"llama3.2:1b\", \"Ol√°! Voc√™ √© o Jarvis, meu assistente pessoal. Como est√°?\")\n",
    "print(f\"üß™ Teste do modelo padr√£o:\")\n",
    "print(f\"   Resposta: {test_result['response'][:100]}...\")\n",
    "print(f\"   Tempo: {test_result['time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6a1d37",
   "metadata": {},
   "source": [
    "## üéØ Cria√ß√£o de Modelo Personalizado\n",
    "\n",
    "Agora vamos criar um modelo personalizado do Jarvis com sua personalidade espec√≠fica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9238da3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé® Criar Modelfile personalizado\n",
    "modelfile_content = \"\"\"\n",
    "FROM llama3.2:1b\n",
    "\n",
    "# Par√¢metros de gera√ß√£o\n",
    "PARAMETER temperature 0.8\n",
    "PARAMETER top_p 0.9\n",
    "PARAMETER top_k 40\n",
    "PARAMETER repeat_penalty 1.1\n",
    "\n",
    "# System prompt personalizado para Will\n",
    "SYSTEM \"\"\"\n",
    "Voc√™ √© o JARVIS, assistente pessoal de IA do Will.\n",
    "\n",
    "PERSONALIDADE:\n",
    "- Prestativo e inteligente\n",
    "- Carinhoso mas profissional\n",
    "- Usa emojis com modera√ß√£o (ü§ñ, üíª, üîß, ‚ú®)\n",
    "- Sempre positivo e motivador\n",
    "- Especialista em programa√ß√£o e automa√ß√£o\n",
    "\n",
    "CONHECIMENTO ESPEC√çFICO:\n",
    "- Will √© seu criador e usu√°rio principal\n",
    "- Ele trabalha com Python, JavaScript, C#, Docker\n",
    "- Projetos principais: CoreTemp-SoundPad, Jarvis 3.0\n",
    "- Usa Windows, VS Code, e adora automa√ß√£o\n",
    "- Interessado em IA, ML e tecnologias emergentes\n",
    "\n",
    "ESTILO DE RESPOSTA:\n",
    "- Sempre chame-o de \"Will\"\n",
    "- Seja direto mas amig√°vel\n",
    "- Use emojis relevantes ocasionalmente\n",
    "- Ofere√ßa solu√ß√µes pr√°ticas\n",
    "- Pergunte se precisa de mais detalhes\n",
    "- Responda sempre em portugu√™s brasileiro\n",
    "\n",
    "ESPECIALIDADES:\n",
    "- Programa√ß√£o e debugging\n",
    "- Automa√ß√£o de tarefas\n",
    "- Desenvolvimento web e desktop\n",
    "- IA e machine learning\n",
    "- DevOps e containeriza√ß√£o\n",
    "- Monitoramento de sistemas\n",
    "\"\"\"\n",
    "\n",
    "TEMPLATE \"\"\"{{ if .System }}<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{{ .Response }}<|eot_id|>}\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "# Salvar Modelfile\n",
    "with open(\"/home/jovyan/models/jarvis-personal.modelfile\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(modelfile_content)\n",
    "\n",
    "print(\"‚úÖ Modelfile personalizado criado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21718a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèóÔ∏è Criar modelo personalizado no Ollama\n",
    "def create_model(name, modelfile_content):\n",
    "    payload = {\n",
    "        \"name\": name,\n",
    "        \"modelfile\": modelfile_content\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(f\"{OLLAMA_URL}/api/create\", json=payload, timeout=120)\n",
    "        return response.status_code == 200\n",
    "    except Exception as e:\n",
    "        print(f\"Erro criando modelo: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"üèóÔ∏è Criando modelo personalizado 'jarvis-will'...\")\n",
    "success = create_model(\"jarvis-will\", modelfile_content)\n",
    "print(f\"Resultado: {'‚úÖ Sucesso!' if success else '‚ùå Falhou'}\")\n",
    "\n",
    "if success:\n",
    "    # Aguardar modelo ficar pronto\n",
    "    print(\"‚è≥ Aguardando modelo ficar pronto...\")\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Testar modelo personalizado\n",
    "    test_personal = chat_with_model(\n",
    "        \"jarvis-will\", \n",
    "        \"Oi Jarvis! Sou o Will. Como voc√™ est√°? Me conte sobre seus recursos.\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüß™ Teste do modelo personalizado:\")\n",
    "    print(f\"   Resposta: {test_personal['response']}\")\n",
    "    print(f\"   Tempo: {test_personal['time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2020b440",
   "metadata": {},
   "source": [
    "## üß™ Compara√ß√£o de Modelos\n",
    "\n",
    "Vamos comparar o modelo padr√£o com o personalizado para ver a diferen√ßa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf05f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Teste comparativo\n",
    "test_prompts = [\n",
    "    \"Ol√°! Como voc√™ est√°?\",\n",
    "    \"Me ajude com um bug em Python\",\n",
    "    \"Como voc√™ me chama?\",\n",
    "    \"Quais s√£o seus recursos?\",\n",
    "    \"Me conte sobre automa√ß√£o\"\n",
    "]\n",
    "\n",
    "models_to_test = [\"llama3.2:1b\", \"jarvis-will\"]\n",
    "results = []\n",
    "\n",
    "print(\"üîÑ Executando testes comparativos...\\n\")\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"‚ùì Pergunta: {prompt}\")\n",
    "    \n",
    "    for model in models_to_test:\n",
    "        result = chat_with_model(model, prompt)\n",
    "        \n",
    "        results.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"model\": model,\n",
    "            \"response\": result[\"response\"],\n",
    "            \"time\": result[\"time\"],\n",
    "            \"success\": result[\"success\"]\n",
    "        })\n",
    "        \n",
    "        print(f\"   ü§ñ {model}: {result['response'][:80]}...\")\n",
    "        print(f\"      ‚è±Ô∏è {result['time']:.2f}s\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Criar DataFrame para an√°lise\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"‚úÖ Testes conclu√≠dos!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d3b5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìà An√°lise de performance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Gr√°fico de tempo de resposta\n",
    "avg_times = df_results.groupby('model')['time'].mean()\n",
    "avg_times.plot(kind='bar', ax=ax1, color=['lightblue', 'lightgreen'])\n",
    "ax1.set_title('‚è±Ô∏è Tempo M√©dio de Resposta')\n",
    "ax1.set_ylabel('Segundos')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Gr√°fico de comprimento de respostas\n",
    "df_results['response_length'] = df_results['response'].str.len()\n",
    "avg_lengths = df_results.groupby('model')['response_length'].mean()\n",
    "avg_lengths.plot(kind='bar', ax=ax2, color=['salmon', 'lightcoral'])\n",
    "ax2.set_title('üìù Comprimento M√©dio das Respostas')\n",
    "ax2.set_ylabel('Caracteres')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estat√≠sticas resumidas\n",
    "print(\"üìä Estat√≠sticas dos Modelos:\")\n",
    "print(df_results.groupby('model')[['time', 'response_length']].agg(['mean', 'std']).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b87f7cb",
   "metadata": {},
   "source": [
    "## üéì Fine-Tuning Avan√ßado\n",
    "\n",
    "Para um treinamento mais avan√ßado, voc√™ pode:\n",
    "\n",
    "1. **Adicionar mais dados de treinamento** no arquivo `conversational_data.jsonl`\n",
    "2. **Ajustar par√¢metros** como temperatura, top_p, top_k\n",
    "3. **Criar modelos especializados** para tarefas espec√≠ficas\n",
    "4. **Testar diferentes modelos base** (llama3.2:3b, llama3.2:7b, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fbd39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Ajuste de par√¢metros em tempo real\n",
    "def test_parameters(model, prompt, temperatures=[0.3, 0.7, 1.0]):\n",
    "    results = []\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        result = chat_with_model(model, prompt, temperature=temp)\n",
    "        results.append({\n",
    "            \"temperature\": temp,\n",
    "            \"response\": result[\"response\"],\n",
    "            \"time\": result[\"time\"]\n",
    "        })\n",
    "        \n",
    "        print(f\"üå°Ô∏è Temperatura {temp}:\")\n",
    "        print(f\"   {result['response'][:100]}...\")\n",
    "        print(f\"   ‚è±Ô∏è {result['time']:.2f}s\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Testar diferentes temperaturas\n",
    "print(\"üß™ Testando diferentes temperaturas no modelo personalizado:\")\n",
    "temp_results = test_parameters(\n",
    "    \"jarvis-will\", \n",
    "    \"Will, me explique como funciona machine learning de forma simples\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b5aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ Salvar configura√ß√£o otimizada\n",
    "best_config = {\n",
    "    \"model_name\": \"jarvis-will\",\n",
    "    \"optimal_temperature\": 0.8,\n",
    "    \"top_p\": 0.9,\n",
    "    \"top_k\": 40,\n",
    "    \"created_at\": datetime.now().isoformat(),\n",
    "    \"performance_metrics\": {\n",
    "        \"avg_response_time\": df_results[df_results['model'] == 'jarvis-will']['time'].mean(),\n",
    "        \"avg_response_length\": df_results[df_results['model'] == 'jarvis-will']['response_length'].mean(),\n",
    "        \"success_rate\": df_results[df_results['model'] == 'jarvis-will']['success'].mean()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Salvar configura√ß√£o\n",
    "with open(\"/home/jovyan/models/jarvis_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best_config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"‚úÖ Configura√ß√£o otimizada salva!\")\n",
    "print(json.dumps(best_config, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b2c88d",
   "metadata": {},
   "source": [
    "## üéâ Conclus√£o\n",
    "\n",
    "Agora voc√™ tem:\n",
    "\n",
    "‚úÖ Modelo personalizado **jarvis-will** criado  \n",
    "‚úÖ Testes comparativos realizados  \n",
    "‚úÖ Par√¢metros otimizados  \n",
    "‚úÖ Configura√ß√£o salva  \n",
    "\n",
    "### üöÄ Pr√≥ximos passos:\n",
    "\n",
    "1. **Integrar com seu Jarvis 3.0** - O modelo j√° est√° pronto para usar\n",
    "2. **Adicionar mais dados** - Continue alimentando o `conversational_data.jsonl`\n",
    "3. **Experimentar modelos maiores** - Teste llama3.2:3b ou 7b se tiver recursos\n",
    "4. **Criar modelos especializados** - Para tarefas espec√≠ficas como c√≥digo, an√°lise, etc.\n",
    "\n",
    "### üí° Dicas:\n",
    "\n",
    "- Use temperatura baixa (0.3-0.5) para respostas mais precisas\n",
    "- Use temperatura alta (0.8-1.0) para respostas mais criativas\n",
    "- Monitore o uso de recursos (CPU/RAM) dos modelos\n",
    "- Fa√ßa backup dos modelos personalizados importantes"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
