"""
Sistema de IA do JARVIS 3.0
Suporte para OpenAI e Ollama Local
"""

import openai
import json
from typing import Dict, List, Optional, Any
from datetime import datetime
import asyncio
from utils.logger_fixed import setup_logger
from core.plugins import plugin_manager
from core.ollama_integration import OllamaLocalAI

class AIEngine:
    """Motor de IA principal do JARVIS"""
    
    DEFAULT_OLLAMA_MODEL = "llama3.2:1b"
    
    def __init__(self, ai_config):
        self.config = ai_config
        from utils.logger_fixed import default_logger
        self.logger = default_logger
        
        # Configura√ß√£o da API OpenAI
        if self.config.api_key:
            openai.api_key = self.config.api_key
        
        # Inicializar Ollama Local
        self.ollama = OllamaLocalAI()
        self.use_local_ai = False
        
        # Verificar se Ollama est√° dispon√≠vel
        if self.ollama.check_connection():
            self.use_local_ai = True
            self.logger.info("üß† Ollama Local AI detectado e ativo!")
            
            # Tentar criar modelo personalizado Jarvis
            self._setup_jarvis_model()
        else:
            self.logger.info("Ollama n√£o detectado. Usando modo API ou fallback.")
        
        # Personalidades dispon√≠veis
        self.personalities = {
            "assistente": {
                "name": "Assistente Profissional",
                "description": "Assistente formal e direto",
                "system_prompt": "Voc√™ √© JARVIS, um assistente virtual profissional e eficiente. Seja direto, claro e √∫til.",
                "ollama_model": "jarvis-personal"  # Modelo personalizado
            },
            "amigavel": {
                "name": "Amig√°vel",
                "description": "Assistente descontra√≠do e amig√°vel",
                "system_prompt": "Voc√™ √© JARVIS, um assistente virtual amig√°vel e descontra√≠do. Use um tom casual e seja prestativo.",
                "ollama_model": self.DEFAULT_OLLAMA_MODEL
            },
            "tecnico": {
                "name": "T√©cnico",
                "description": "Especialista em tecnologia",
                "system_prompt": "Voc√™ √© JARVIS, um assistente t√©cnico especializado. Forne√ßa informa√ß√µes detalhadas e precisas sobre tecnologia.",
                "ollama_model": self.DEFAULT_OLLAMA_MODEL
            }
        }
        
        self.current_personality = "assistente"
        self.conversation_history = []
        
    def set_personality(self, personality: str):
        """Define a personalidade da IA"""
        if personality in self.personalities:
            self.current_personality = personality
            self.logger.info(f"Personalidade alterada para: {self.personalities[personality]['name']}")
        else:
            self.logger.warning(f"Personalidade '{personality}' n√£o encontrada")
    
    def get_system_prompt(self) -> str:
        """Retorna o prompt do sistema baseado na personalidade atual"""
        base_prompt = self.personalities[self.current_personality]["system_prompt"]
        
        # Adiciona informa√ß√µes do sistema
        system_info = f"""
        
Informa√ß√µes do sistema:
- Data/Hora atual: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}
- Sistema: Windows
- Voc√™ pode executar comandos do sistema e monitorar recursos
- Voc√™ tem acesso a funcionalidades de automa√ß√£o e controle

Responda sempre em portugu√™s brasileiro.
"""
        
        return base_prompt + system_info
    
    async def chat(self, message: str, context: Optional[Dict] = None) -> str:
        """
        Processa uma mensagem de chat
        
        Args:
            message: Mensagem do usu√°rio
            context: Contexto adicional (dados do sistema, etc.)
        
        Returns:
            Resposta da IA
        """
        try:
            self.logger.info(f"üí¨ Processando mensagem: {message[:50]}...")
            
            # CORRE√á√ÉO: Validar entrada
            if not message or not message.strip():
                return "Por favor, digite uma mensagem v√°lida."
            
            # CORRE√á√ÉO: Prioridade melhorada e fallback garantido
            response = None
            
            # 1. Tentar Ollama Local primeiro
            if self.use_local_ai and self.ollama.check_connection():
                self.logger.info("üß† Usando Ollama Local")
                try:
                    response = await self._call_ollama_local(message, context)
                except Exception as e:
                    self.logger.error(f"Falha no Ollama: {e}")
                    response = None
            
            # 2. Fallback para OpenAI se dispon√≠vel
            if not response and self.config.api_key:
                self.logger.info("üåê Fallback para OpenAI API")
                try:
                    response = await self._call_openai_api_with_context(message, context)
                except Exception as e:
                    self.logger.error(f"Falha na OpenAI: {e}")
                    response = None
            
            # 3. Fallback local se tudo falhar
            if not response:
                self.logger.warning("‚ö†Ô∏è Usando resposta local (fallback)")
                response = self._local_response(message, context)
            
            # CORRE√á√ÉO: Validar resposta final
            if not response or not response.strip():
                response = "Desculpe, n√£o consegui processar sua mensagem no momento. Tente novamente."
            
            # Salva no hist√≥rico
            self.conversation_history.append({"role": "user", "content": message})
            self.conversation_history.append({"role": "assistant", "content": response})
            
            # Limita o hist√≥rico a 20 mensagens para melhor performance
            if len(self.conversation_history) > 20:
                self.conversation_history = self.conversation_history[-20:]
            
            self.logger.info("Resposta gerada com sucesso")
            return response
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro cr√≠tico no chat: {e}")
            return "Desculpe, ocorreu um erro cr√≠tico. O sistema est√° se recuperando. Tente novamente em alguns segundos."
    
    async def _call_ollama_local(self, message: str, context: Optional[Dict] = None) -> str:
        """Chama Ollama Local AI de forma ass√≠ncrona"""
        try:
            # Usar modelo espec√≠fico da personalidade
            current_model = self.personalities[self.current_personality].get("ollama_model", self.DEFAULT_OLLAMA_MODEL)
            self.ollama.set_model(current_model)
            
            # Construir prompt com contexto
            system_prompt = self.get_system_prompt()
            
            if context:
                context_str = f"\n\nContexto do sistema: {json.dumps(context, ensure_ascii=False, indent=2)}"
                system_prompt += context_str
            
            # Incluir hist√≥rico recente no prompt
            conversation_context = ""
            if self.conversation_history:
                recent_history = self.conversation_history[-4:]  # CORRE√á√ÉO: Reduzido para 4 mensagens
                for msg in recent_history:
                    role = "Usu√°rio" if msg["role"] == "user" else "Jarvis"
                    conversation_context += f"\n{role}: {msg['content'][:100]}"  # Limite de 100 chars
            
            # Prompt final mais compacto
            full_prompt = f"{system_prompt}\n\nHist√≥rico:{conversation_context}\n\nUsu√°rio: {message}\nJarvis:"
            
            # CORRE√á√ÉO: Executar em thread para n√£o bloquear
            import concurrent.futures
            
            loop = asyncio.get_event_loop()
            with concurrent.futures.ThreadPoolExecutor() as executor:
                response = await loop.run_in_executor(
                    executor, 
                    self.ollama.chat,
                    full_prompt
                )
            
            # Limpar poss√≠veis artefatos do prompt
            if response.startswith("Jarvis:"):
                response = response[7:].strip()
            
            return response
            
        except Exception as e:
            self.logger.error(f"Erro no Ollama: {e}")
            return self._fallback_response()
    
    async def _call_openai_api_with_context(self, message: str, context: Optional[Dict] = None) -> str:
        """Chama a API da OpenAI com contexto"""
        try:
            # Constr√≥i o hist√≥rico da conversa
            messages = [
                {"role": "system", "content": self.get_system_prompt()}
            ]
            
            # Adiciona contexto se fornecido
            if context:
                context_str = f"Contexto do sistema: {json.dumps(context, ensure_ascii=False, indent=2)}"
                messages.append({"role": "system", "content": context_str})
            
            # Adiciona hist√≥rico recente
            messages.extend(self.conversation_history[-10:])  # √öltimas 10 mensagens
            
            # Adiciona mensagem atual
            messages.append({"role": "user", "content": message})
            
            response = await openai.ChatCompletion.acreate(
                model=self.config.ai.model_name,
                messages=messages,
                max_tokens=self.config.ai.max_tokens,
                temperature=self.config.ai.temperature,
                stream=False
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            self.logger.error(f"Erro na API OpenAI: {e}")
            return self._fallback_response()
    
    def _fallback_response(self) -> str:
        """Resposta de fallback quando a API falha"""
        responses = [
            "Estou com dificuldades para processar sua solicita√ß√£o no momento. Pode tentar novamente?",
            "Desculpe, n√£o consigo acessar minha IA principal agora. Posso ajudar com comandos b√°sicos do sistema.",
            "Parece que h√° um problema tempor√°rio com minha conex√£o. Que tal verificar o status do sistema?",
        ]
        
        import random
        return random.choice(responses)
    
    def analyze_command(self, text: str) -> Dict[str, Any]:
        """
        Analisa um texto para identificar comandos
        
        Args:
            text: Texto a ser analisado
            
        Returns:
            Dicion√°rio com informa√ß√µes do comando
        """
        text_lower = text.lower()
        
        # Comandos de sistema
        if any(word in text_lower for word in ['cpu', 'processador', 'performance']):
            return {
                'type': 'system_info',
                'action': 'cpu_info',
                'confidence': 0.8
            }
        
        if any(word in text_lower for word in ['memoria', 'ram', 'mem√≥ria']):
            return {
                'type': 'system_info',
                'action': 'memory_info',
                'confidence': 0.8
            }
        
        # Comandos de plugins
        if any(word in text_lower for word in ['anotar', 'nota', 'anota√ß√£o']):
            return {
                'type': 'plugin',
                'action': 'add_note',
                'plugin': 'notes',
                'confidence': 0.9
            }
        
        if any(word in text_lower for word in ['lembrar', 'lembrete', 'alarme']):
            return {
                'type': 'plugin',
                'action': 'add_reminder',
                'plugin': 'reminders',
                'confidence': 0.9
            }
        
        if any(word in text_lower for word in ['tarefa', 'task', 'fazer']):
            return {
                'type': 'plugin',
                'action': 'add_task',
                'plugin': 'tasks',
                'confidence': 0.8
            }
        
        if any(word in text_lower for word in ['disco', 'armazenamento', 'hd', 'ssd']):
            return {
                'type': 'system_info',
                'action': 'disk_info',
                'confidence': 0.8
            }
        
        if any(word in text_lower for word in ['bateria', 'energia']):
            return {
                'type': 'system_info',
                'action': 'battery_info',
                'confidence': 0.8
            }
        
        if any(word in text_lower for word in ['temperatura', 'temp']):
            return {
                'type': 'system_info',
                'action': 'temperature_info',
                'confidence': 0.8
            }
        
        # Comandos de aplica√ß√£o
        if any(word in text_lower for word in ['abrir', 'executar', 'rodar']):
            return {
                'type': 'app_control',
                'action': 'open_app',
                'confidence': 0.7
            }
        
        # Comando de chat geral
        return {
            'type': 'chat',
            'action': 'general_chat',
            'confidence': 0.5
        }
    
    def clear_history(self):
        """Limpa o hist√≥rico da conversa"""
        self.conversation_history = []
        self.logger.info("Hist√≥rico de conversa limpo")
    
    def get_conversation_summary(self) -> Dict:
        """Retorna um resumo da conversa"""
        return {
            'total_messages': len(self.conversation_history),
            'current_personality': self.personalities[self.current_personality]['name'],
            'last_message_time': datetime.now().isoformat() if self.conversation_history else None
        }
    
    def _local_response(self, message: str, context: Dict = None) -> str:
        """Sistema de resposta local quando n√£o h√° API key"""
        return self._get_system_response(message, context) or self._get_greeting_response() or self._get_default_response()
    
    def _get_system_response(self, message: str, context: Dict = None) -> Optional[str]:
        """Respostas relacionadas ao sistema"""
        message_lower = message.lower()
        
        if any(word in message_lower for word in ['cpu', 'processador']):
            if context and 'cpu_percent' in context:
                cpu_val = context['cpu_percent']
                status = "alto" if cpu_val > 80 else "normal"
                return f"CPU: {cpu_val:.1f}% - Status: {status}"
            return "Monitoramento de CPU dispon√≠vel no dashboard."
        
        if any(word in message_lower for word in ['memoria', 'ram']):
            if context and 'memory_percent' in context:
                mem_val = context['memory_percent']
                status = "alto" if mem_val > 80 else "normal"
                return f"Mem√≥ria: {mem_val:.1f}% - Status: {status}"
            return "Monitoramento de mem√≥ria dispon√≠vel no dashboard."
        
        if any(word in message_lower for word in ['sistema', 'status']):
            if context:
                cpu = context.get('cpu_percent', 0)
                mem = context.get('memory_percent', 0)
                status = "normal" if cpu < 70 and mem < 70 else "alta utiliza√ß√£o"
                return f"Sistema: {status}. CPU: {cpu:.1f}%, RAM: {mem:.1f}%"
            return "Sistema operacional. Dashboard ativo."
        
        return None
    
    def _get_greeting_response(self) -> Optional[str]:
        """Respostas de sauda√ß√£o baseadas na personalidade"""
        personality = self.current_personality
        
        greetings = {
            "assistente": "Ol√°! Como posso ajud√°-lo hoje?",
            "tecnico": "Sistema pronto. Aguardando consultas t√©cnicas.",
            "amigavel": "Oi! Tudo bem? Como posso te ajudar?"
        }
        
        return greetings.get(personality)
    
    def _get_default_response(self) -> str:
        """Resposta padr√£o quando nenhuma IA est√° dispon√≠vel"""
        responses = [
            "Sistema em modo local. Funcionalidades limitadas dispon√≠veis.",
            "IA principal indispon√≠vel. Usando respostas locais.",
            "Acesse o dashboard para monitoramento em tempo real."
        ]
        
        import random
        return random.choice(responses)
    def _setup_jarvis_model(self):
        """Configura o modelo personalizado do Jarvis"""
        try:
            # Verificar se modelo jarvis-personal j√° existe
            models = self.ollama.list_models()
            model_names = [m["name"] for m in models]
            
            if "jarvis-personal" not in model_names:
                # Criar modelo personalizado
                modelfile_path = "training_data/Modelfile"
                if self.ollama.create_custom_model("jarvis-personal", modelfile_path):
                    self.logger.info("‚úÖ Modelo Jarvis personalizado criado!")
                else:
                    self.logger.warning("‚ö†Ô∏è Erro criando modelo personalizado. Usando modelo padr√£o.")
            else:
                self.logger.info("‚úÖ Modelo Jarvis personalizado j√° existe!")
                
        except Exception as e:
            self.logger.error(f"Erro configurando modelo Jarvis: {e}")
    
    def switch_ai_mode(self, mode: str = "auto"):
        """
        Alterna entre modos de IA
        
        Args:
            mode: 'local', 'api', 'auto'
        """
        if mode == "local" and self.ollama.check_connection():
            self.use_local_ai = True
            self.logger.info("üß† Modo: IA Local (Ollama)")
        elif mode == "api" and self.config.api_key:
            self.use_local_ai = False
            self.logger.info("üåê Modo: API OpenAI")
        elif mode == "auto":
            # Auto: prefere local se dispon√≠vel, sen√£o API
            if self.ollama.check_connection():
                self.use_local_ai = True
                self.logger.info("üß† Modo: Auto -> IA Local")
            elif self.config.api_key:
                self.use_local_ai = False
                self.logger.info("üåê Modo: Auto -> API OpenAI")
            else:
                self.logger.warning("‚ö†Ô∏è Modo: Fallback local")
    
    def get_ai_status(self) -> Dict:
        """Retorna status das IAs dispon√≠veis"""
        return {
            "ollama_available": self.ollama.check_connection(),
            "openai_available": bool(self.config.api_key),
            "current_mode": "local" if self.use_local_ai else "api",
            "ollama_models": self.ollama.list_models() if self.ollama.check_connection() else [],
            "current_model": self.personalities[self.current_personality].get("ollama_model", "N/A")
        }
